#!/usr/bin/env python3

import argparse
import os
import subprocess
import sys
import urllib.request
from pathlib import Path


class LiteRTLMCLI:
    def __init__(self):
        self.script_dir = Path(__file__).parent.resolve()
        self.models_dir = self.script_dir / "models"
        self.litert_lm_dir = self.script_dir / "LiteRT-LM"
        self.bazelisk = self.script_dir / "bazelisk"

    def download_model(self, model_name):
        """Download model from HuggingFace if not already present."""
        model_path = self.models_dir / model_name

        # Handle special filename mappings for certain models
        if model_name == "litert-community/gemma-3-270m-it":
            model_filename = "gemma3-270m-it-q8.litertlm"
        else:
            model_filename = f"{model_name.split('/')[-1]}.litertlm"

        model_file = model_path / model_filename

        if model_file.exists():
            print(f"✓ Model already exists at {model_file}")
            return model_file

        # Create model directory
        model_path.mkdir(parents=True, exist_ok=True)

        # Construct HuggingFace URL
        hf_url = f"https://huggingface.co/{model_name}/resolve/main/{model_filename}"

        print(f"Downloading model from {hf_url}...")
        print(f"Saving to {model_file}...")

        try:
            # Check for HF_TOKEN environment variable for gated models
            hf_token = os.environ.get('HF_TOKEN')

            # Fallback to reading token from huggingface-cli cache
            if not hf_token:
                token_path = Path.home() / '.cache' / 'huggingface' / 'token'
                if token_path.exists():
                    hf_token = token_path.read_text().strip()
                    print("Using HuggingFace token from ~/.cache/huggingface/token")

            # Download with progress
            def progress_hook(block_num, block_size, total_size):
                if total_size > 0:
                    percent = min(100, block_num * block_size * 100 / total_size)
                    sys.stdout.write(f"\rProgress: {percent:.1f}%")
                    sys.stdout.flush()

            # Add authentication header if token is available
            if hf_token:
                request = urllib.request.Request(hf_url)
                request.add_header('Authorization', f'Bearer {hf_token}')
                print("Using HF_TOKEN for authentication")

                # For authenticated downloads, we need to handle it differently
                with urllib.request.urlopen(request) as response:
                    total_size = int(response.headers.get('Content-Length', 0))
                    block_size = 8192
                    downloaded = 0

                    with open(model_file, 'wb') as f:
                        while True:
                            buffer = response.read(block_size)
                            if not buffer:
                                break
                            downloaded += len(buffer)
                            f.write(buffer)
                            if total_size > 0:
                                percent = min(100, downloaded * 100 / total_size)
                                sys.stdout.write(f"\rProgress: {percent:.1f}%")
                                sys.stdout.flush()
            else:
                urllib.request.urlretrieve(hf_url, model_file, reporthook=progress_hook)

            print("\n✓ Model downloaded successfully")
            return model_file
        except urllib.error.HTTPError as e:
            if e.code == 401:
                print(f"\n✗ Error: Unauthorized (401). This model requires authentication.")
                print("\nPlease authenticate with HuggingFace:")
                print("  1. Run: hf login")
                print("  2. Enter your HuggingFace token (get it from https://huggingface.co/settings/tokens)")
                print("\nIf you've already logged in, your token should be automatically detected.")
                print("Alternatively, set the environment variable:")
                print("  export HF_TOKEN=$(cat ~/.cache/huggingface/token)")
                print(f"\nFor gated models like {model_name}, you may also need to:")
                print(f"  - Visit https://huggingface.co/{model_name}")
                print("  - Click 'Agree and access repository'")
                print("  - Wait a few minutes for access to be granted")
            else:
                print(f"\n✗ HTTP Error downloading model: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"\n✗ Error downloading model: {e}")
            sys.exit(1)

    def build_litert_lm(self, android=False):
        """Build the LiteRT-LM binary using Bazel."""
        if not self.litert_lm_dir.exists():
            print("✗ Error: LiteRT-LM directory not found. Please run ./setup first.")
            sys.exit(1)

        if not self.bazelisk.exists():
            print("✗ Error: Bazelisk not found. Please run ./setup first.")
            sys.exit(1)

        if android:
            return self._build_android()
        else:
            return self._build_host()

    def _build_host(self):
        """Build for host (Linux/Mac)."""
        binary_path = self.litert_lm_dir / "bazel-bin" / "runtime" / "engine" / "litert_lm_advanced_main"

        # Check if binary already exists
        if binary_path.exists():
            print("✓ Binary already built")
            return binary_path

        print("Building LiteRT-LM binary (this may take a while)...")
        try:
            subprocess.run(
                [str(self.bazelisk), "build", "runtime/engine:litert_lm_advanced_main"],
                cwd=self.litert_lm_dir,
                check=True
            )
            print("✓ Build successful")
            return binary_path
        except subprocess.CalledProcessError as e:
            print(f"✗ Build failed: {e}")
            sys.exit(1)

    def _build_android(self):
        """Build for Android."""
        # Check for ANDROID_NDK_HOME
        if not os.environ.get("ANDROID_NDK_HOME"):
            print("✗ Error: ANDROID_NDK_HOME environment variable not set.")
            print("Please set it to your Android NDK path.")
            sys.exit(1)

        output_base = Path.home() / ".cache" / "bazel_android_out"
        print(f"Building for Android (arm64)...")
        print(f"Using output base: {output_base}")

        try:
            # Build command
            subprocess.run(
                [
                    str(self.bazelisk),
                    f"--output_base={output_base}",
                    "build",
                    "--config=android_arm64",
                    "//runtime/engine:litert_lm_advanced_main"
                ],
                cwd=self.litert_lm_dir,
                check=True
            )
            
            # Find artifact path
            # We can use 'bazel info' with the same flags to find the actual bazel-bin
            result = subprocess.run(
                [
                    str(self.bazelisk),
                    f"--output_base={output_base}",
                    "info",
                    "--config=android_arm64",
                    "bazel-bin"
                ],
                cwd=self.litert_lm_dir,
                check=True,
                capture_output=True,
                text=True
            )
            
            bazel_bin_dir = Path(result.stdout.strip())
            android_binary = bazel_bin_dir / "runtime" / "engine" / "litert_lm_advanced_main"
            
            if android_binary.exists():
                print("✓ Android build successful")
                return android_binary
            else:
                print(f"✗ Build succeeded but binary not found at expected path: {android_binary}")
                sys.exit(1)

        except subprocess.CalledProcessError as e:
            print(f"✗ Android build failed: {e}")
            sys.exit(1)

    def is_preconverted_model(self, model_name):
        """Check if this is a pre-converted model from litert-community."""
        return model_name.startswith("litert-community/")

    def is_convertible_model(self, model_name):
        """Check if this model can be converted from HuggingFace."""
        # Pre-converted models are not convertible
        if self.is_preconverted_model(model_name):
            return False

        # Check for Qwen3 models
        if "Qwen3" in model_name or "qwen3" in model_name.lower():
            if any(size in model_name for size in ["0.6B", "1.7B", "4B"]):
                return True

        # Check for Gemma3 models
        if "gemma-3" in model_name.lower() or "gemma3" in model_name.lower():
            if any(size in model_name.lower() for size in ["270m", "1b"]):
                return True

        return False

    def get_model_file_path(self, model_name):
        """Get the expected path for the model's .litertlm file."""
        model_path = self.models_dir / model_name

        # Handle special filename mappings
        if model_name == "litert-community/gemma-3-270m-it":
            model_filename = "gemma3-270m-it-q8.litertlm"
        else:
            model_filename = f"{model_name.split('/')[-1]}.litertlm"

        return model_path / model_filename

    def run_model(self, model_name, android=False):
        """Run the model with user input."""
        # Check if model file exists
        model_file = self.get_model_file_path(model_name)

        if not model_file.exists():
            # For pre-converted models, try to download
            if self.is_preconverted_model(model_name):
                print(f"Model not found locally. Downloading from HuggingFace...")
                model_file = self.download_model(model_name)

            # For convertible models, auto-convert
            elif self.is_convertible_model(model_name):
                print(f"Model not found locally. Converting from HuggingFace...")
                print(f"This will download ~1-2GB and take several minutes.\n")

                # Trigger conversion with default parameters
                self.convert_model(
                    model_name=model_name,
                    output_dir=None,
                    quantize="dynamic_int8",
                    prefill_seq_len=256,
                    kv_cache_max_len=1280
                )

                # Check if conversion succeeded
                if not model_file.exists():
                    print(f"✗ Error: Conversion completed but model file not found at {model_file}")
                    sys.exit(1)

            else:
                print(f"✗ Error: Unsupported model: {model_name}")
                print("\nSupported models:")
                print("  Pre-converted:")
                print("    - litert-community/Qwen3-0.6B")
                print("    - litert-community/gemma-3-270m-it")
                print("  Convertible:")
                print("    - Qwen/Qwen3-0.6B, Qwen/Qwen3-1.7B, Qwen/Qwen3-4B")
                print("    - google/gemma-3-270m, google/gemma-3-1b")
                sys.exit(1)
        else:
            print(f"✓ Model already exists at {model_file}")

        # Build binary
        binary_path = self.build_litert_lm(android=android)

        if android:
            self._run_android_inference(binary_path, model_file)
            return

        # Get user input
        print("\nEnter your prompt:")
        input_prompt = input("> ")

        if not input_prompt.strip():
            print("✗ Error: Empty prompt")
            sys.exit(1)

        # Run the binary
        print("\nRunning inference...\n")
        try:
            subprocess.run(
                [
                    str(binary_path),
                    "--model_path", str(model_file),
                    "--input_prompt", input_prompt,
                    "--backend", "cpu"
                ],
                check=True
            )
        except subprocess.CalledProcessError as e:
            print(f"\n✗ Inference failed: {e}")
            sys.exit(1)
        except KeyboardInterrupt:
            print("\n\nInterrupted by user")
            sys.exit(0)

    def _get_adb_devices(self):
        """Get list of connected ADB devices."""
        try:
            result = subprocess.run(["adb", "devices"], capture_output=True, text=True, check=True)
            lines = result.stdout.strip().split("\n")[1:] # Skip header
            devices = []
            for line in lines:
                if line.strip():
                    parts = line.split("\t")
                    if len(parts) >= 2 and parts[1] == "device":
                        devices.append(parts[0])
            return devices
        except FileNotFoundError:
            print("✗ Error: 'adb' command not found. Please install Android Platform Tools.")
            sys.exit(1)

    def _run_android_inference(self, binary_path, model_path):
        """Push artifacts and run inference on Android device."""
        
        # 1. Select Device
        devices = self._get_adb_devices()
        if not devices:
            print("✗ Error: No Android devices found via adb.")
            print("Please connect a device and enable USB Debugging.")
            sys.exit(1)
            
        if len(devices) == 1:
            device_serial = devices[0]
            print(f"Using device: {device_serial}")
        else:
            print("\nMultiple devices found:")
            for i, dev in enumerate(devices):
                print(f"  {i+1}) {dev}")
            
            while True:
                try:
                    selection = input("\nSelect device number: ")
                    idx = int(selection) - 1
                    if 0 <= idx < len(devices):
                        device_serial = devices[idx]
                        break
                    print("Invalid selection.")
                except ValueError:
                    print("Invalid input.")

        # 2. Push Binary
        target_bin_path = "/data/local/tmp/litert_lm_advanced_main"
        print(f"\nPushing binary to {target_bin_path}...")
        try:
            subprocess.run(["adb", "-s", device_serial, "push", str(binary_path), target_bin_path], check=True)
            subprocess.run(["adb", "-s", device_serial, "shell", f"chmod +x {target_bin_path}"], check=True)
        except subprocess.CalledProcessError:
            print("✗ Error pushing binary.")
            sys.exit(1)

        # 3. Push Model
        model_filename = model_path.name
        target_model_path = f"/data/local/tmp/{model_filename}"
        
        # Check if model exists and compare size (simple optimization)
        needs_push = True
        try:
            # Get remote size
            res = subprocess.run(
                ["adb", "-s", device_serial, "shell", f"ls -l {target_model_path}"],
                capture_output=True, text=True
            )
            if res.returncode == 0:
                # Basic check: if file exists, ask user? or just overwrite? 
                # For large models, checking size matches local might be good enough to skip
                # Parsing 'ls -l' output can be brittle across android versions/shells (toybox etc)
                # Let's just assume if it exists we might want to skip or ask. 
                # Using 'adb shell stat -c %s' is better but not always available.
                # Let's simple ask user if they want to skip if file exists, or just push.
                # User asked to "push the desired file", implying ensure it's there.
                # I'll implement a simple check: "File exists. Overwrite? [y/N]"
                print(f"\nModel file {model_filename} already exists on device.")
                should_push = input("Push again? (y/N): ").lower().startswith('y')
                if not should_push:
                    needs_push = False
        except:
            pass # File likely doesn't exist

        if needs_push:
            print(f"Pushing model to {target_model_path} (this may take time)...")
            try:
                subprocess.run(["adb", "-s", device_serial, "push", str(model_path), target_model_path], check=True)
            except subprocess.CalledProcessError:
                print("✗ Error pushing model.")
                sys.exit(1)
        else:
            print("Skipping model push.")

        # 4. Get Prompt
        print("\nEnter your prompt:")
        input_prompt = input("> ")
        if not input_prompt.strip():
            print("✗ Error: Empty prompt")
            sys.exit(1)

        # 5. Run Inference
        print("\nRunning inference on Android (benchmark mode)...")
        print("="*50)
        cmd = [
            "adb", "-s", device_serial, "shell",
            f"{target_bin_path} --backend=cpu --model_path={target_model_path} --input_prompt=\"{input_prompt}\" --benchmark"
        ]
        
        try:
            # Use subprocess.call so output streams directly to terminal
            subprocess.run(cmd, check=True)
        except subprocess.CalledProcessError as e:
            print(f"\n✗ Inference failed on device: {e}")
            sys.exit(1)
        except KeyboardInterrupt:
            print("\nInterrupted.")
            sys.exit(0)


    def convert_model(self, model_name: str, output_dir=None, quantize="dynamic_int8",
                      prefill_seq_len=256, kv_cache_max_len=1024):
        """Convert a model to LiteRT-LM format"""

        # Detect model type and size
        if "Qwen3" in model_name or "qwen3" in model_name.lower():
            from converter.qwen3_converter import Qwen3Converter

            # Extract model size (0.6B -> 0.6b)
            if "0.6B" in model_name:
                model_size = "0.6b"
            elif "1.7B" in model_name:
                model_size = "1.7b"
            elif "4B" in model_name:
                model_size = "4b"
            else:
                print("✗ Error: Cannot determine model size from name")
                print("Supported: Qwen/Qwen3-0.6B, Qwen/Qwen3-1.7B, Qwen/Qwen3-4B")
                sys.exit(1)

            converter = Qwen3Converter(model_name, self.script_dir, model_size)

        elif "gemma-3" in model_name.lower() or "gemma3" in model_name.lower():
            from converter.gemma3_converter import Gemma3Converter

            # Extract model size
            if "270m" in model_name.lower():
                model_size = "270m"
            elif "1b" in model_name.lower():
                model_size = "1b"
            else:
                print("✗ Error: Cannot determine model size from name")
                print("Supported: google/gemma-3-270m, google/gemma-3-1b")
                sys.exit(1)

            converter = Gemma3Converter(model_name, self.script_dir, model_size)

        else:
            print(f"✗ Error: Unsupported model: {model_name}")
            print("Currently supported:")
            print("  - Qwen/Qwen3-0.6B, Qwen/Qwen3-1.7B, Qwen/Qwen3-4B")
            print("  - google/gemma-3-270m, google/gemma-3-1b")
            sys.exit(1)

        # Execute conversion
        try:
            output_path = Path(output_dir) if output_dir else None
            converter.convert(
                quantize=quantize,
                prefill_seq_len=prefill_seq_len,
                kv_cache_max_len=kv_cache_max_len,
                output_dir=output_path
            )
        except Exception as e:
            print(f"\n✗ Conversion failed: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="CLI to use Google AI Edge stack for LLMs",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Run command
    run_parser = subparsers.add_parser("run", help="Run inference with a model")
    run_parser.add_argument("model", help="Model name (e.g., litert-community/Qwen3-0.6B, litert-community/gemma-3-270m-it)")
    run_parser.add_argument("--android", action="store_true", help="Build for Android and output binary location")

    # Convert command
    convert_parser = subparsers.add_parser("convert", help="Convert a model to LiteRT-LM format")
    convert_parser.add_argument("model", help="Model name (e.g., Qwen/Qwen3-0.6B, google/gemma-3-270m)")
    convert_parser.add_argument("--output-dir", help="Custom output directory (optional)")
    convert_parser.add_argument("--quantize", default="dynamic_int8",
                              help="Quantization type (default: dynamic_int8)")
    convert_parser.add_argument("--prefill-seq-len", type=int, default=256,
                              help="Prefill sequence length (default: 256, will use multiple values)")
    convert_parser.add_argument("--kv-cache-max-len", type=int, default=1280,
                              help="KV cache max length (default: 1280)")

    args = parser.parse_args()

    if args.command == "run":
        cli = LiteRTLMCLI()
        cli.run_model(args.model, android=args.android)
    elif args.command == "convert":
        cli = LiteRTLMCLI()
        cli.convert_model(
            args.model,
            output_dir=args.output_dir,
            quantize=args.quantize,
            prefill_seq_len=args.prefill_seq_len,
            kv_cache_max_len=args.kv_cache_max_len
        )
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
